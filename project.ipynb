{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an MLP with 2 inputs, 3-4+ hidden units and one output on the following examples (XOR function):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) Build a perceptron\n",
    "A stack of perceptrons together == hidden layer a.k.a a dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, X, y, hidden_units=3, epochs=100, learning_rate=.1, random_state=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.hidden_units = hidden_units\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def checkParameters(self):\n",
    "        self.num_inputs = self.X.shape[1]\n",
    "        self.num_outputs = self.y.shape[1]\n",
    "        if self.X.shape[0] != self.y.shape[0]:\n",
    "            raise ValueError('X and Y have mismatched shapes')\n",
    "        if self.num_inputs < 1:\n",
    "            raise ValueError('Must have at least 1 input')\n",
    "        if self.num_outputs < 1:\n",
    "            raise ValueError('Must have at least 1 output')\n",
    "        if self.hidden_units < 1:\n",
    "            raise ValueError('Must have at least 1 hidden unit')\n",
    "        if self.epochs < 1:\n",
    "            raise ValueError('Must train for at least 1 epoch')\n",
    "        #Is it a max of 1 also??\n",
    "        if self.learning_rate <=0:\n",
    "            raise ValueError('Learning rate must be greater than 0')\n",
    "    \n",
    "    def tan_h_deriv(self, x):\n",
    "        return 1 - (x**2)\n",
    "    def sigmoid(self, x, deriv=False):\n",
    "        sig = (1 / (1 + np.exp(-x)))\n",
    "        if deriv is True:\n",
    "            return x * (1-x)\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "     \n",
    "    def myLogLoss(self, deriv=False):\n",
    "        #epsilon is to avoid divide by zero errors\n",
    "        epsilon = 1e-5\n",
    "        if deriv is True:\n",
    "            return -(self.y/self.output) + ((1-self.y)/(1-self.output))\n",
    "        return (1/self.num_inputs) * np.sum(-((self.y * np.log(self.output+epsilon)) + (1 - self.y)  * np.log(1 - self.output + epsilon)))\n",
    "    \n",
    " \n",
    "    def forward(self):\n",
    "        self.z1 = self.X@self.w1 + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = self.a1@self.w2 + self.b2\n",
    "        self.output = self.sigmoid(self.z2)\n",
    "   \n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss = self.myLogLoss()\n",
    "        d_output = self.myLogLoss(deriv=True) * self.sigmoid(self.output, deriv=True)\n",
    "        d_w2 = (1/self.num_inputs) * (self.a1.T @ d_output)\n",
    "        d_b2 = (1/self.num_inputs) * (np.sum(d_output, axis = 0, keepdims=True))\n",
    "        d_z1 = (d_output@self.w2.T) * self.tan_h_deriv(self.a1)\n",
    "        d_w1 = (1/self.num_inputs) * (self.X.T@d_z1)\n",
    "        d_b1 = (1/self.num_inputs) * (np.sum(d_z1, axis=0, keepdims=True))\n",
    "\n",
    "        self.w1 -= (self.learning_rate * d_w1)\n",
    "        self.w2 -= (self.learning_rate * d_w2)\n",
    "        self.b1 -= (self.learning_rate * d_b1)\n",
    "        self.b2 -= (self.learning_rate * d_b2)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.checkParameters()\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        #Xavier initialization to train model to converge faster\n",
    "        self.w1 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.num_inputs+self.hidden_units)),\n",
    "                                size = (self.num_inputs, self.hidden_units))\n",
    "        self.w2 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.hidden_units+self.num_outputs)),\n",
    "                                size = (self.hidden_units, self.num_outputs))\n",
    "        self.b1 = np.zeros((1,self.hidden_units))       \n",
    "        self.b2 = np.zeros((1,self.num_outputs))\n",
    "        for i in range (self.epochs):\n",
    "            self.forward()\n",
    "        #    print(\"Epoch {} Loss is {}\".format(i+1, self.loss))\n",
    "            self.backward()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.X = x\n",
    "        self.forward()\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(([0, 1],[1,0], [1,1], [0,0]))\n",
    "y = np.array(([1],[1],[0],[0]))\n",
    "test = MLP(x,y,4,random_state=0, epochs=100000)\n",
    "test.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99985229],\n",
       "       [0.99998968],\n",
       "       [0.00014796],\n",
       "       [0.00009289]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "test.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9983956 , 0.99978067, 0.00163647])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([6.43340103, 8.42472798, -6.41357507])\n",
    "sigmoid(-7.04127255)\n",
    "sigmoid(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv is True:\n",
    "        return x * (1-x)\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_data = []\n",
    "for n in range(100):\n",
    "    data.append(np.random.uniform(low=-1.0,high=1.0, size=(4, 1)))\n",
    "    y_data.append(np.sin(data[-1][0] - data[-1][1] + data[-1][2] - data[-1][3]))\n",
    "y_data = np.array(y_data)\n",
    "X_data = np.array(data)\n",
    "X_data = X_data.reshape(100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0, test_size = 0.25, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998459796152872\n",
      "-0.9996706301937582\n"
     ]
    }
   ],
   "source": [
    "print(np.max(y_data))\n",
    "print(np.min(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, X, y, hidden_units=3, epochs=100, learning_rate=.1, random_state=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.hidden_units = hidden_units\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def checkParameters(self):\n",
    "        self.num_inputs = self.X.shape[1]\n",
    "        self.num_outputs = self.y.shape[1]\n",
    "        if self.X.shape[0] != self.y.shape[0]:\n",
    "            raise ValueError('X and Y have mismatched shapes')\n",
    "        if self.num_inputs < 1:\n",
    "            raise ValueError('Must have at least 1 input')\n",
    "        if self.num_outputs < 1:\n",
    "            raise ValueError('Must have at least 1 output')\n",
    "        if self.hidden_units < 1:\n",
    "            raise ValueError('Must have at least 1 hidden unit')\n",
    "        if self.epochs < 1:\n",
    "            raise ValueError('Must train for at least 1 epoch')\n",
    "        #Is it a max of 1 also??\n",
    "        if self.learning_rate <=0:\n",
    "            raise ValueError('Learning rate must be greater than 0')\n",
    "\n",
    "\n",
    "    def sigmoid(self, x, deriv=False):\n",
    "        if deriv is True:\n",
    "            return x * (1-x)\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def myLogLoss(self, deriv=False):\n",
    "        #epsilon is to avoid divide by zero errors\n",
    "        epsilon = 1e-5\n",
    "    \n",
    "        if deriv is True:\n",
    "            return -(self.y/self.output) + ((1-self.y)/(1-self.output))\n",
    "        return (1/self.num_inputs) * np.sum(-((self.y * np.log(self.output+epsilon)) + (1 - self.y)  * np.log(1 - self.output + epsilon)))\n",
    "    \n",
    "    def MSE(self, deriv=False):\n",
    "        if deriv == True:\n",
    "            return -2 * (self.y - self.output)\n",
    "        return (np.square(self.y - self.output).mean())\n",
    "\n",
    "    def forward(self):\n",
    "        self.z1 = self.X@self.w1 + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = self.a1@self.w2 + self.b2\n",
    "        self.output = np.tanh(self.z2)\n",
    "    \n",
    "    def tan_h_deriv(self, x):\n",
    "        return 1 - (x**2)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss = self.MSE()\n",
    "        #added the times F'() see results four. Without it resutlts1\n",
    "      #  d_output = (self.output - self.y) * (1-(self.output**2))\n",
    "        d_output = self.MSE(deriv=True) * self.tan_h_deriv(self.output)\n",
    "        d_w2 = (1/self.num_inputs) * (self.a1.T @ d_output)\n",
    "        d_b2 = (1/self.num_inputs) * (np.sum(d_output, axis = 0, keepdims=True))\n",
    "        d_z1 = (d_output@self.w2.T) * self.tan_h_deriv(self.a1)\n",
    "        d_w1 = (1/self.num_inputs) * (self.X.T@d_z1)\n",
    "        d_b1 = (1/self.num_inputs) * (np.sum(d_z1, axis=0, keepdims=True))\n",
    "\n",
    "        self.w1 -= (self.learning_rate * d_w1)\n",
    "        self.w2 -= (self.learning_rate * d_w2)\n",
    "        self.b1 -= (self.learning_rate * d_b1)\n",
    "        self.b2 -= (self.learning_rate * d_b2)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.checkParameters()\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "#         Xavier initialization to train model to converge faster\n",
    "        self.w1 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.num_inputs+self.hidden_units)),\n",
    "                                size = (self.num_inputs, self.hidden_units))\n",
    "        self.w2 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.hidden_units+self.num_outputs)),\n",
    "                                size = (self.hidden_units, self.num_outputs))\n",
    "        self.b1 = np.zeros((1,self.hidden_units))\n",
    "        self.b2 = np.zeros((1,self.num_outputs))\n",
    "        for i in range (self.epochs):\n",
    "            self.forward()\n",
    "        #    print(\"Epoch {} Loss is {}\".format(i+1, self.loss))\n",
    "            self.backward()\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.X = x\n",
    "        self.forward()\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6 = pd.DataFrame(index=['epochs', 'hidden units', 'train_error', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results4 = pd.DataFrame(index=['epochs', 'hidden units', 'train_error', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(index=['epochs', 'hidden units', 'train_error', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame(index=['epochs', 'hidden units', 'train_error', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = pd.DataFrame(index=['epochs', 'hidden units', 'train_error', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSE(predict, y):\n",
    "    return np.power((predict - y), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epochs = [1000, 10000, 100000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model 1\n",
      "testing model 2\n",
      "testing model 3\n",
      "testing model 4\n",
      "testing model 5\n",
      "testing model 6\n",
      "testing model 7\n",
      "testing model 8\n",
      "testing model 9\n",
      "testing model 10\n",
      "testing model 11\n",
      "testing model 12\n",
      "testing model 13\n",
      "testing model 14\n",
      "testing model 15\n",
      "testing model 16\n"
     ]
    }
   ],
   "source": [
    "model = 1\n",
    "for epoch in test_epochs:\n",
    "    for hidden in range(2, 6):\n",
    "        print('testing model {}'.format(model))\n",
    "        test = MLP(X_train,y_train,hidden,random_state=0, epochs=epoch)\n",
    "        test.fit(X_train, y_train)\n",
    "        train_error = SSE(test.predict(X_train), y_train)\n",
    "        test_error = SSE(test.predict(X_test), y_test)\n",
    "        results6[model] = [epoch, hidden, train_error, test_error]\n",
    "        model += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden units</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_error</th>\n",
       "      <td>0.025814</td>\n",
       "      <td>0.02937</td>\n",
       "      <td>0.015715</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.020232</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.016511</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.026067</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.015274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_error</th>\n",
       "      <td>0.042875</td>\n",
       "      <td>0.04252</td>\n",
       "      <td>0.038239</td>\n",
       "      <td>0.029322</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.049897</td>\n",
       "      <td>0.051454</td>\n",
       "      <td>0.044841</td>\n",
       "      <td>0.038910</td>\n",
       "      <td>0.057722</td>\n",
       "      <td>0.065705</td>\n",
       "      <td>0.063738</td>\n",
       "      <td>0.038916</td>\n",
       "      <td>0.086603</td>\n",
       "      <td>0.070086</td>\n",
       "      <td>0.063890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1           2            3            4             5   \\\n",
       "epochs        1000.000000  1000.00000  1000.000000  1000.000000  10000.000000   \n",
       "hidden units     2.000000     3.00000     4.000000     5.000000      2.000000   \n",
       "train_error      0.025814     0.02937     0.015715     0.018677      0.020232   \n",
       "test_error       0.042875     0.04252     0.038239     0.029322      0.038879   \n",
       "\n",
       "                        6             7             8              9   \\\n",
       "epochs        10000.000000  10000.000000  10000.000000  100000.000000   \n",
       "hidden units      3.000000      4.000000      5.000000       2.000000   \n",
       "train_error       0.029754      0.009606      0.016511       0.020245   \n",
       "test_error        0.049897      0.051454      0.044841       0.038910   \n",
       "\n",
       "                         10             11             12              13  \\\n",
       "epochs        100000.000000  100000.000000  100000.000000  1000000.000000   \n",
       "hidden units       3.000000       4.000000       5.000000        2.000000   \n",
       "train_error        0.030341       0.006701       0.018379        0.020250   \n",
       "test_error         0.057722       0.065705       0.063738        0.038916   \n",
       "\n",
       "                          14              15              16  \n",
       "epochs        1000000.000000  1000000.000000  1000000.000000  \n",
       "hidden units        3.000000        4.000000        5.000000  \n",
       "train_error         0.026067        0.006490        0.015274  \n",
       "test_error          0.086603        0.070086        0.063890  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden units</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_error</th>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.020393</td>\n",
       "      <td>0.018232</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.008561</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>0.017248</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.008962</td>\n",
       "      <td>0.008278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_error</th>\n",
       "      <td>0.035976</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.019646</td>\n",
       "      <td>0.031125</td>\n",
       "      <td>0.040651</td>\n",
       "      <td>0.030903</td>\n",
       "      <td>0.039211</td>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.036524</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.040174</td>\n",
       "      <td>0.073451</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.037489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1            2            3            4   \\\n",
       "epochs        1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "hidden units     2.000000     3.000000     4.000000     5.000000   \n",
       "train_error      0.020846     0.020371     0.012607     0.020393   \n",
       "test_error       0.035976     0.033296     0.019646     0.031125   \n",
       "\n",
       "                        5             6             7             8   \\\n",
       "epochs        10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "hidden units      2.000000      3.000000      4.000000      5.000000   \n",
       "train_error       0.018232      0.012038      0.008561      0.009684   \n",
       "test_error        0.040651      0.030903      0.039211      0.027347   \n",
       "\n",
       "                         9              10             11             12  \\\n",
       "epochs        100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "hidden units       2.000000       3.000000       4.000000       5.000000   \n",
       "train_error        0.017248       0.009807       0.008554       0.005275   \n",
       "test_error         0.040230       0.036524       0.046355       0.025387   \n",
       "\n",
       "                          13              14              15              16  \n",
       "epochs        1000000.000000  1000000.000000  1000000.000000  1000000.000000  \n",
       "hidden units        2.000000        3.000000        4.000000        5.000000  \n",
       "train_error         0.017128        0.009162        0.008962        0.008278  \n",
       "test_error          0.040174        0.073451        0.048800        0.037489  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden units</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_error</th>\n",
       "      <td>0.020790</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.021659</td>\n",
       "      <td>0.020244</td>\n",
       "      <td>0.020394</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>0.014162</td>\n",
       "      <td>0.015391</td>\n",
       "      <td>0.021239</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.021239</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.016973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_error</th>\n",
       "      <td>0.036388</td>\n",
       "      <td>0.037338</td>\n",
       "      <td>0.038756</td>\n",
       "      <td>0.047729</td>\n",
       "      <td>0.037215</td>\n",
       "      <td>0.036336</td>\n",
       "      <td>0.093349</td>\n",
       "      <td>0.067977</td>\n",
       "      <td>0.037460</td>\n",
       "      <td>0.037888</td>\n",
       "      <td>0.159024</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.037460</td>\n",
       "      <td>0.037601</td>\n",
       "      <td>0.157637</td>\n",
       "      <td>0.081951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1            2            3            4   \\\n",
       "epochs        1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "hidden units     2.000000     3.000000     4.000000     5.000000   \n",
       "train_error      0.020790     0.020523     0.021659     0.020244   \n",
       "test_error       0.036388     0.037338     0.038756     0.047729   \n",
       "\n",
       "                        5             6             7             8   \\\n",
       "epochs        10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "hidden units      2.000000      3.000000      4.000000      5.000000   \n",
       "train_error       0.020394      0.020078      0.014162      0.015391   \n",
       "test_error        0.037215      0.036336      0.093349      0.067977   \n",
       "\n",
       "                         9              10             11             12  \\\n",
       "epochs        100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "hidden units       2.000000       3.000000       4.000000       5.000000   \n",
       "train_error        0.021239       0.020149       0.020396       0.017172   \n",
       "test_error         0.037460       0.037888       0.159024       0.078390   \n",
       "\n",
       "                          13              14              15              16  \n",
       "epochs        1000000.000000  1000000.000000  1000000.000000  1000000.000000  \n",
       "hidden units        2.000000        3.000000        4.000000        5.000000  \n",
       "train_error         0.021239        0.019867        0.022054        0.016973  \n",
       "test_error          0.037460        0.037601        0.157637        0.081951  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySoftMax(X):\n",
    "    return np.exp(X)/(np.sum(np.exp(X), axis=1, keepdims=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_inputs, num_outputs,\n",
    "                 hidden_units=3, epochs=100, learning_rate=.1,\n",
    "                 random_state=None, loss='log',\n",
    "                 first_activation='sig', second_activation='sig'):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.hidden_units = hidden_units\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.loss_type = loss\n",
    "        self.a_func_1 = first_activation\n",
    "        self.a_func_2 = second_activation\n",
    "        \n",
    "    def checkParameters(self):\n",
    "        self.num_inputs = self.X.shape[1]\n",
    "        self.num_outputs = self.y.shape[1]\n",
    "        if self.X.shape[0] != self.y.shape[0]:\n",
    "            raise ValueError('X and Y have mismatched shapes')\n",
    "        if self.num_inputs < 1:\n",
    "            raise ValueError('Must have at least 1 input')\n",
    "        if self.num_outputs < 1:\n",
    "            raise ValueError('Must have at least 1 output')\n",
    "        if self.hidden_units < 1:\n",
    "            raise ValueError('Must have at least 1 hidden unit')\n",
    "        if self.epochs < 1:\n",
    "            raise ValueError('Must train for at least 1 epoch')\n",
    "        #Is it a max of 1 also??\n",
    "        if self.learning_rate <=0:\n",
    "            raise ValueError('Learning rate must be greater than 0')\n",
    "        if self.loss_type not in ['log', 'mse']:\n",
    "            raise ValueError('Unknown loss function {}'.format(self.loss_type))\n",
    "        if self.a_func_1 not in ['sig', 'tanh']:\n",
    "            raise ValueError('Unknown activation function {}'.format(self.a_func_1))\n",
    "        if self.a_func_2 not in ['sig', 'tanh']:\n",
    "            raise ValueError('Unknown activation function {}'.format(self.a_func_2))\n",
    "\n",
    "        \n",
    "    def myActivation(self, active):\n",
    "        if active == 'sig':\n",
    "            def active(x, deriv=False):\n",
    "                print('sig')\n",
    "                sig = (1 / (1 + np.exp(-x)))\n",
    "                if deriv is True:\n",
    "                    return sig * (1-sig)\n",
    "                return sig\n",
    "        else:\n",
    "            def active(x, deriv=False):\n",
    "                print('tanh')\n",
    "                if deriv is True:\n",
    "                    return 1 - (x**2)\n",
    "                return np.tanh(x)\n",
    "        return active\n",
    "\n",
    "    def myLoss(self, loss):\n",
    "        if loss == 'log':\n",
    "            def loss(deriv=False):\n",
    "                epsilon = 1e-5\n",
    "                if deriv is True:\n",
    "                    return -(self.y/self.output) + ((1-self.y)/(1-self.output))\n",
    "                return (1/self.num_inputs) * np.sum(-((self.y * np.log(self.output+epsilon)) + (1 - self.y)  * np.log(1 - self.output + epsilon)))\n",
    "        else:\n",
    "            def loss(deriv=False):\n",
    "                if deriv == True:\n",
    "                    return -2 * (self.y - self.output)\n",
    "                return (np.square(self.y - self.output).mean())\n",
    "        return loss\n",
    "        \n",
    "    def mLoss(self, deriv=False):\n",
    "        #epsilon is to avoid divide by zero errors\n",
    "        epsilon = 1e-5\n",
    "    \n",
    "        if deriv is True:\n",
    "            return -(self.y/self.output) + ((1-self.y)/(1-self.output))\n",
    "        return (1/self.num_inputs) * np.sum(-((self.y * np.log(self.output+epsilon)) + (1 - self.y)  * np.log(1 - self.output + epsilon)))\n",
    "    \n",
    "\n",
    "    def forward(self):\n",
    "        self.z1 = self.X@self.w1 + self.b1\n",
    "        self.a1 = self.a_func_1(self.z1)\n",
    "        self.z2 = self.a1@self.w2 + self.b2\n",
    "        self.output = self.a_func_2(self.z2)\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss = self.myLossFunction()\n",
    "        #added the times F'() see results four. Without it resutlts1\n",
    "      #  d_output = (self.output - self.y) * (1-(self.output**2))\n",
    "        d_output = self.MSE(deriv=True) * self.a_func_2(self.output, deriv=True)\n",
    "        d_w2 = (1/self.num_inputs) * (self.a1.T @ d_output)\n",
    "        d_b2 = (1/self.num_inputs) * (np.sum(d_output, axis = 0, keepdims=True))\n",
    "        d_z1 = (d_output@self.w2.T) * self.a_func_1(self.a1, deriv=True)\n",
    "        d_w1 = (1/self.num_inputs) * (self.X.T@d_z1)\n",
    "        d_b1 = (1/self.num_inputs) * (np.sum(d_z1, axis=0, keepdims=True))\n",
    "\n",
    "        self.w1 -= (self.learning_rate * d_w1)\n",
    "        self.w2 -= (self.learning_rate * d_w2)\n",
    "        self.b1 -= (self.learning_rate * d_b1)\n",
    "        self.b2 -= (self.learning_rate * d_b2)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.X = x\n",
    "        self.y = y\n",
    "        self.checkParameters()\n",
    "        self.myLossFunction = self.myLoss(self.loss_type)\n",
    "        self.a_func_1 = self.myActivation(self.a_func_1)\n",
    "        self.a_func_2 = self.myActivation(self.a_func_2)\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "#         Xavier initialization to train model to converge faster\n",
    "        self.w1 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.num_inputs+self.hidden_units)),\n",
    "                                size = (self.num_inputs, self.hidden_units))\n",
    "        self.w2 = np.random.normal(loc=0.0,\n",
    "                                scale=np.sqrt(2/(self.hidden_units+self.num_outputs)),\n",
    "                                size = (self.hidden_units, self.num_outputs))\n",
    "        self.b1 = np.zeros((1,self.hidden_units))\n",
    "        self.b2 = np.zeros((1,self.num_outputs))\n",
    "        for i in range (self.epochs):\n",
    "            self.forward()\n",
    "        #    print(\"Epoch {} Loss is {}\".format(i+1, self.loss))\n",
    "            self.backward()\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.X = x\n",
    "        self.forward()\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MLP(X_train.shape[0], y_train.shape[0], 3, 1, .1, loss='mse', \n",
    "           first_activation = 'tanh', second_activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n",
      "tanh\n",
      "tanh\n",
      "tanh\n"
     ]
    }
   ],
   "source": [
    "test.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n",
      "tanh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.6528469 ],\n",
       "       [-0.55568389],\n",
       "       [-0.35485113],\n",
       "       [ 0.38638498],\n",
       "       [ 0.69233975],\n",
       "       [ 0.70093031],\n",
       "       [ 0.56821487],\n",
       "       [ 0.41246322],\n",
       "       [ 0.30726114],\n",
       "       [-0.94058008],\n",
       "       [-0.03027645],\n",
       "       [ 0.83782261],\n",
       "       [-0.23349321],\n",
       "       [ 0.72257247],\n",
       "       [ 0.71763994],\n",
       "       [-0.18329858],\n",
       "       [ 0.06743704],\n",
       "       [-0.8959221 ],\n",
       "       [-0.77839625],\n",
       "       [ 0.11023348],\n",
       "       [ 0.3407258 ],\n",
       "       [-0.76412524],\n",
       "       [-0.92066867],\n",
       "       [-0.72654014],\n",
       "       [-0.48625013]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
